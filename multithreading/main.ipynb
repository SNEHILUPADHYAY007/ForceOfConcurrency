{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5520a520-fb5a-4a8d-b382-2f73a6e42de9",
   "metadata": {},
   "source": [
    "# Basic Imports and useful Informations\n",
    "- API:- https://swapi.dev/api/\n",
    "- Repo Link:- https://github.com/SNEHILUPADHYAY007/ForceOfConcurrency/\n",
    "- Root Path:- D:\\data-engineering-projects\\ForceOfConcurrency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70200e83-27ce-46d5-8d0e-3c0b41efcdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import *\n",
    "from time import *\n",
    "from queue import *\n",
    "import os\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342dc912-2ea2-4e25-88a6-2565b26fa1a0",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "240e0c02-e4e4-417a-8a6d-022bd2be3ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes path exist\n",
      "DATA fetched successfully\n",
      "Starting Creating the JSON file for respective sources\n",
      "file writing for people started\n",
      "file writing for planets started\n",
      "file writing for films started\n",
      "file writing for species started\n",
      "file writing for vehicles started\n",
      "file writing for starships started\n",
      "Everything Completed Hence Exiting!!!\n"
     ]
    }
   ],
   "source": [
    "keys = []\n",
    "root_path = 'D:\\\\data-engineering-projects\\\\ForceOfConcurrency'\n",
    "api_path = 'https://swapi.dev/api'\n",
    "data_dict = {}\n",
    "url = {}\n",
    "\n",
    "# Function to get the initial data from the API\n",
    "def get_initial_data():\n",
    "    try:\n",
    "        # GET the initial info from the API\n",
    "        with requests.get(api_path) as req:\n",
    "            if req.status_code == 200:\n",
    "                # Storing the result in a list\n",
    "                global keys\n",
    "                keys = list(req.json().keys()).copy()\n",
    "    \n",
    "        # preparing the dict to hold the json data for respective keys\n",
    "        for key in keys:\n",
    "            data_dict[key] = []\n",
    "            url[key] = []\n",
    "    except Exception as e:\n",
    "        print(\"Some Exception occured while getting initial data\",e)\n",
    "        \n",
    "# Creating the Required folders if not exist\n",
    "def create_folders():\n",
    "    # Creating required directory and folders for storing the results\n",
    "    if os.path.isdir(f'{root_path}\\\\data'):\n",
    "        print('Yes path exist')\n",
    "    else:\n",
    "        print('Path donot exist...we need to create it')\n",
    "        os.mkdir(f'{root_path}\\\\data')\n",
    "        \n",
    "        for api_paths in keys:\n",
    "            os.mkdir(f'{root_path}\\\\data\\\\{api_paths}')\n",
    "        \n",
    "        print('Paths successfully created!!!')\n",
    "        \n",
    "# Get single page response and append in respective list\n",
    "def get_single_page_data(url, endpoint):\n",
    "    try:\n",
    "        with requests.get(url) as req:\n",
    "            if req.status_code == 200:\n",
    "                data = req.json()\n",
    "                data_dict[endpoint].append(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Some Exception occured while reading the data for {url}:{e}\")\n",
    "        \n",
    "# Function to get the data from the API\n",
    "def get_data(endpoint):\n",
    "    pages = 0\n",
    "    try:\n",
    "        # Calculating Total Pages for each API Endpoint\n",
    "        req = requests.get(f'{api_path}/{endpoint}')\n",
    "        if req.status_code == 200:\n",
    "            req = req.json()\n",
    "            total_pages = divmod(req['count'], len(req['results']))\n",
    "            if total_pages[1] > 0:\n",
    "                pages = total_pages[0] + 1\n",
    "            else:\n",
    "                pages = total_pages[0]\n",
    "    \n",
    "        source = [f'{api_path}/{endpoint}/?page={page_no}' for page_no in range(1, pages + 1)]\n",
    "        url[endpoint] = source\n",
    "        \n",
    "        # If wanted to fetch using single thread / API Endpoint. \n",
    "        # for i in url:\n",
    "        #     get_single_page_data(i, endpoint)\n",
    "        # print(url)\n",
    "    except Exception as e:\n",
    "        print(f\"Some Exception occured while preparing the urls for {endpoint}:{e}\")\n",
    "\n",
    "# Function to create the files for respective sources and storing in the respective folder path\n",
    "def file_op(source):\n",
    "    print(f\"file writing for {source} started\")\n",
    "    file_path = f\"{root_path}\\\\data\\\\{source}\\\\{source}.json\"\n",
    "    try:\n",
    "        with open(file_path, 'w') as f:\n",
    "            f.write(json.dumps(data_dict[source]))\n",
    "    except Exception as e:\n",
    "        print(\"Some error occured while writing the file\",e)\n",
    "    \n",
    "def main(): \n",
    "    # Getting initial data from the API\n",
    "    get_initial_data()\n",
    "\n",
    "    # Creating the required files to store the files\n",
    "    create_folders()\n",
    "\n",
    "    # Creating Parent threads for each endpoint.\n",
    "    # Each Thread is responsible to produce the API URLS for their respective endpoint\n",
    "    threads = []\n",
    "    for i in keys:\n",
    "        th = Thread(target = get_data, args=(i,), name = f\"thread_{i}\")\n",
    "        threads.append(th)\n",
    "        th.start()\n",
    "    \n",
    "    # Making sure main doesn't stop until these threads complete the execution\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "   \n",
    "    # Creating Child threads for respective endpoint.\n",
    "    # Child threads == pages(calculated above)\n",
    "    # Each child thread will be pulling the different url concurrently and storing the result in the dict against respective key\n",
    "    child_threads = []\n",
    "    cnt = 0\n",
    "    for key, value in url.items():\n",
    "        urls = url[key]\n",
    "        for u in urls:\n",
    "            child_th = Thread(target = get_single_page_data, args = (u, key), name = (f\"child_thread_key_{key}\"))\n",
    "            child_threads.append(child_th)\n",
    "            child_th.start()\n",
    "        \n",
    "    # Making sure main doesn't stop until these threads complete the execution\n",
    "    for i in child_threads:\n",
    "        i.join()\n",
    "\n",
    "    print(\"DATA fetched successfully\")\n",
    "    print(\"Starting Creating the JSON file for respective sources\")\n",
    "\n",
    "    file_op_threads = []\n",
    "    for i in keys:\n",
    "        file_op_th = Thread(target = file_op, args = (i,), name = (f\"file_op_{key}\"))\n",
    "        file_op_threads.append(file_op_th)\n",
    "        file_op_th.start()\n",
    "\n",
    "    # Making sure main doesn't stop until these threads complete the execution\n",
    "    for i in file_op_threads:\n",
    "        i.join()\n",
    "        \n",
    "\n",
    "# Calling the main function\n",
    "main()\n",
    "# print(data_dict)\n",
    "print(\"Everything Completed Hence Exiting!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ef7885-e999-44c8-94ff-e5fe08638334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2169fd78-19fe-4b6b-bebd-8d14f1b6aa5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
